

```
---

## docs/10-performance-tuning.md

```markdown
# 10 · 吞吐与显存优化（Cookbook + 原理小抄）

> 目标：给一套“**先做什么，怎么调，为什么**”的实战策略。核心对象：多轮 + 可选工具的 PPO 训练（SGLang 后端），以及单轮对比（vLLM）。

---

## 0）性能模型（一分钟直觉）

- **显存三巨头**：序列长度（prompt + 历史 + 生成）、batch（微批 × 并行度）、KV Cache。  
- **两种阶段**：  
  - **Prefill**（把上下文过一遍）→ 受序列长影响大；  
  - **Decode**（逐 token 生）→ 受并发与 KV 命中影响。  
- **多轮**会让 Prefill 更重（历史累加），所以**控制上下文长度**比单轮更关键。

---

## 1）调参顺序（金标准 10 步）

1. **先收敛**：`*_micro_batch=1`，`max_prompt/response` 从 384/128 起。  
2. **KL 护栏**：`kl_coef=1e-3` 起步；reward 噪声大就加到 `2e-3 ~ 5e-3`。  
3. **学习率**：actor `1e-6`，critic `1e-5`；不稳降 2×。  
4. **mini-batch**：先 64，稳定后到 128/256。  
5. **micro-batch**：从 1 提到 2/4；每抬一步观察 OOM 与 SPS。  
6. **长度**：多轮任务把 `max_prompt_length` 卡在 384–768，避免历史无限膨胀；`max_response_length` 控在 64–128。  
7. **SGLang 并发**：确保 rollout 并发度（同批的 prompts 数）与 GPU 利用匹配；不盲目堆长序列。  
8. **工具 I/O**：工具慢 → 用 Ray 并行采样（09 里给了 Actor）；或本地缓存冷门结果。  
9. **vLLM 对比**（可选）：单轮时用 vLLM，`gpu_memory_utilization` 适度提高看看上限。  
10. **日志化**：把 `SPS/reward/KL/loss/json_ok` 同步到 TB/CSV；失败也能复盘。

---

## 2）长度与轮数（最重要的两个旋钮）

- **`data.max_prompt_length`**：多轮模式下，历史越多越占显存；建议**保守**（384–768），并在 prompt 写死“两轮”。  
- **`data.max_response_length`**：越长越慢，且 JSON 结构化任务不需要很长；建议 64–128。  
- **轮数限制**：在奖励里对“>2 轮”直接扣分或判 0；否则训练会学出“爱解释”的坏习惯。

---

## 3）批次与并行（吞吐三件套）

- **微批（显存阀）**：`*_micro_batch_size_per_gpu` 是第一位；OOM 就降。  
- **迷你批（稳定器）**：`ppo_mini_batch_size` 越大越稳但越慢；常见 64/128。  
- **并行度**：`trainer.n_gpus_per_node × trainer.nnodes`；先单机，再多机（09 详述）。

---

## 4）KL 与探索（别压死，也别放飞）

- **不收敛**：KL 太小 → 增到 `2e-3`、`5e-3`。  
- **进步慢**：KL 太大 → 降到 `5e-4`。  
- **注意**：只在**reward 端**或**loss 端**其中一处加 KL，避免双罚。  
- **可加调度**：训练中后期把 KL 逐步变小，释放能力追高分。

---

## 5）SGLang 与 vLLM 的常用开关

- **SGLang**（多轮/工具首选）  
  - 关注**上下文长度**、**并发批大小**、**工具 I/O**。  
  - 如遇“工具触发不稳”，退回最朴素模板，降低温度，或在 prompt 更明确“若需可调用工具，否则直接结论”。

- **vLLM**（单轮对比/高吞吐参考）  
  - `gpu_memory_utilization`：0.3–0.8，越高越吃显存；  
  - `tensor_model_parallel_size`：大模型/多卡才用；  
  - `max_model_len` 与 `block_size` 会影响 KV 利用（具体数值看你的显存与模型）。

---

## 6）显存省法（不动刀也能省）

- **缩长度**（最有效）：`max_prompt/response`。  
- **微批=1**（保险起步）。  
- **模型小一点**（0.5B→1.5B→3B 逐级验证）。  
- **KV 重置**：长对话任务定期清历史（SGLang 会自动以轮为单位组织，尽量不要让“轮内前置文本”过长）。  
- **CPU 侧开销**：日志/打印太多会影响并发（尤其 Python 格式化/正则很重的奖励函数）。

---

## 7）奖励函数也会拖慢（优化技巧）

- 避免**复杂正则**跨行回溯；  
- JSON 解析失败要**就地兜底**，不抛异常；  
- 可以把“格式合规”写成**轻量检测**（例如先找 `{}` 再 `json.loads`，失败就 0）；  
- 评测/训练**共用**同一函数，避免写两套不一致逻辑。

---

## 8）性能排障清单（按优先级排）

1. **是否 OOM/碎片**：稳定复现 → 降微批、降 `gpu_memory_utilization`、降长度、重启进程。  
2. **SPS 不升**：确认 DDP/并发真的生效（GPU 利用率、`nvidia-smi` 进程数）；关掉过高 `save_freq/test_freq`。  
3. **reward 不涨**：先留“主奖”，KL 增大；把“证据/格式” shaping 暂时关掉。  
4. **多轮跑歪**：奖励对非 JSON 记 0；轮数>2 扣分；必要时在两轮之间插系统提醒。  
5. **工具太慢**：Ray 并行采样；工具返回加缓存；证据加分权重减小（鼓励“必要时再用”）。

---

## 9）两个“对照表”，你可以直接贴进文中

### 9.1 小显存（24GB 单卡）推荐起步
| 项 | 值 |
|---|---|
| micro-batch | 1 |
| mini-batch  | 64 |
| prompt/response | 384 / 128 |
| KL | 1e-3 |
| lr actor / critic | 1e-6 / 1e-5 |
| rollout backend | sglang（multi_turn=true） |

### 9.2 标准显存（80GB ×4）推荐进阶
| 项 | 值 |
|---|---|
| n_gpus_per_node | 4 |
| micro-batch | 4 |
| mini-batch | 128–256 |
| prompt/response | 512 / 128 |
| KL | 5e-4 – 1e-3 |
| 额外 | Ray 并行采样 2–4 个 Actor |

---

## 10）“为什么我还慢？”（三个少有人说的点）

- **Python 端瓶颈**：奖励函数/日志/正则/JSON 解析全在 Python，**火焰图**一跑就知道。能下沉到 C/NumPy 就下沉。  
- **工具返回太啰嗦**：证据文本越长，第二轮 Prefill 越慢（尤其多轮）；压缩到摘要。  
- **验证太勤**：`test_freq=1` 会让训练被验证“打断”；新手建议 10–50 步一评。

---

## 11）固定模板：加一页“你的任务专属”优化记录

建议你在仓库 `notebooks/tuning_sps_memory.ipynb` 放一页“调参记录”，包括：
- 配置片段（前后对比）  
- `SPS/显存峰值/JSON 合规率/命中率` 的四格图  
- 三条踩坑与修复总结（给未来的你看）

> 这页图表以后写复现报告（14 篇）直接贴。
```



------



要继续的话，我下一轮可以补 **11「DPO/GRPO 与 RM」** 和 **12「导出与服务（vLLM/SGLang）」**。我会把它们写成：一条最小 DPO/GRPO 训练命令 + 小样本偏好数据 + 与 PPO 的差异；以及 checkpoint 合并、服务端启动、健康检查与 A/B 的整套落地。